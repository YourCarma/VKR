{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef94d0aa-aa09-4c76-8eb2-3aac4f5568a1",
   "metadata": {},
   "source": [
    "# Исследование алгоритмов кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a0035-62ff-424c-8d52-7df71badf9cb",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d7b396-230b-4636-9a7f-25ec1fd8c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "import wget\n",
    "from loguru import logger\n",
    "import tqdm\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim,  logging\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7618da-08dd-443c-a828-49a944afa18f",
   "metadata": {},
   "source": [
    "# Функции предобработки текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd855ab-add3-433e-8f0d-9b643fdbb790",
   "metadata": {},
   "source": [
    "### Токенизация, лемматизация, приведение к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563e1c20-fb1f-4668-a31e-a37b2964c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(\n",
    "                    text, \n",
    "                    nlp,\n",
    "                    punctuation, \n",
    "                    stop_words\n",
    "                    ):\n",
    "    doc = nlp(text.lower())  # Привести к нижнему регистру и лемматизировать\n",
    "    lemmatized_words = [token.lemma_ + \"_\" + token.pos_ for token in doc if token.text not in punctuation and token.text not in stop_words \n",
    "                        and token.pos_ not in [\"PUNCT\", \"SYM\", \"SPACE\"] and len(token.text) > 2]\n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da61a9-90b3-4b03-8f3e-a3883338ffec",
   "metadata": {},
   "source": [
    "### Удаление стоп-слов, пунктуации, возвращает массив обработанных документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829ddbfe-8688-457a-b00d-39f827d9d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "            documents: str\n",
    "            ):\n",
    "    nlp = spacy.load(\"ru_core_news_sm\")\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    processed_text = []\n",
    "    for text in documents:\n",
    "        preprocessed_text = preprocess_text(text, nlp, punctuation, stop_words)\n",
    "        # print(\"Оригинальный текст:\", text)\n",
    "        # print(\"Предобработанный текст:\", preprocessed_text)\n",
    "        # print(\"<----------------------------------------------->\")\n",
    "        processed_text.append(preprocessed_text)\n",
    "    return processed_text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee373d3-c9e1-4fe8-9f90-e5823ec7436c",
   "metadata": {},
   "source": [
    "# Методы векторизации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5566255-9106-49f3-bcd6-dc8a666270eb",
   "metadata": {},
   "source": [
    "## Простые векторные представления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e00b80-5cb3-473a-8a1a-18a9b928ff41",
   "metadata": {},
   "source": [
    "### TF-IDF векторизация документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0d8543-d940-44db-9d93-309f53e1da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(processed_text):\n",
    "    tfidf = TfidfVectorizer(smooth_idf = True, norm = 'l2',  ngram_range = (2,2))  \n",
    "    tfidf_matrix = tfidf.fit_transform(processed_text)\n",
    "    important_words = tfidf.get_feature_names_out()\n",
    "    # print(\"Метод векторизации TF-IDF:\")\n",
    "    # print(\"Важные слова:\", important_words)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265920aa-0cc1-4dad-8586-f3b238df84f8",
   "metadata": {},
   "source": [
    "### Векторизация методом BagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31462cd5-e8cb-48df-a0ef-22c2703e1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(processed_text):\n",
    "    count_vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2,2))\n",
    "    token_counter = count_vectorizer.fit_transform(processed_text)\n",
    "    cv_features = count_vectorizer.get_feature_names_out()\n",
    "    print(\"Метод векторизации BoW:\")\n",
    "    cv_dataframe = pd.DataFrame(token_counter.toarray(), columns = cv_features)\n",
    "    return cv_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633830a6-122b-47c7-aba0-1e89a2f76d26",
   "metadata": {},
   "source": [
    "## Сложные векторные представления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888a3c0-293a-4818-9a5d-74ce792049ee",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0285864-a6d7-48c0-a36d-e2253c7f3894",
   "metadata": {},
   "source": [
    "#### Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af3a882d-05b3-4a97-8104-37b873475c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_vectorizer(\n",
    "                    dataframe, \n",
    "                    model\n",
    "                    ):\n",
    "    model_name = model\n",
    "    dataframe['tokens'] = [doc.split() for doc in tokenize(dataframe.text.tolist())]\n",
    "    tokens_col = dataframe['tokens'].tolist()\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(model_name, binary = True)\n",
    "    features = []\n",
    "    for tokens in tokens_col:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model:\n",
    "                try:\n",
    "                    vectors.append(model[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    dataframe['vectors'] = features\n",
    "    print(\"Метод векторизации Word2Vec\")\n",
    "    print(\"Количество векторов: \", len(features))\n",
    "    print(\"Длина вектора: \", len(features))\n",
    "    print\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af27b3-d823-454a-9c54-4963c817328d",
   "metadata": {},
   "source": [
    "#### Обучениее модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9797f485-3e94-4990-a7a6-e31f0fa6a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_training(dataframe):\n",
    "    logger.debug(\"Токенизация....\")\n",
    "    dataframe['tokens'] = [doc.split() for doc in tokenize(dataframe.text.tolist())]\n",
    "    logger.success(\"Токенизация завершена успешно!\")\n",
    "    tokens_col = dataframe['tokens'].tolist()\n",
    "    #1 - skipgram, 0 -cbow\n",
    "    for algorithm in range(0,1):\n",
    "        if algorithm == 0:\n",
    "            logger.debug(\"Обучение методом CBOW\")\n",
    "            model = gensim.models.Word2Vec(window=40, min_count=2, workers=3, vector_size=300, sg=algorithm)\n",
    "            model.build_vocab(dataframe['tokens'])\n",
    "            logger.debug(f\"Количество строк в корпусе: {model.corpus_count}\")\n",
    "            logger.debug(\"Обучение...\")\n",
    "            model.train(dataframe['tokens'],  epochs=model.epochs, total_examples=model.corpus_count)\n",
    "            logger.success(\"Обучение завершено!\")\n",
    "            model_filename = f\"w2v_cbow_{model.corpus_count}.bin\"\n",
    "            model.wv.save_word2vec_format(model_filename, binary=True)\n",
    "            logger.success(f\"Модель сохранена в {model_filename}\")\n",
    "            \n",
    "        logger.debug(\"Обучение методом SkipGram\")\n",
    "        model = gensim.models.Word2Vec(window=40, min_count=2, workers=3, vector_size=300, sg=algorithm)\n",
    "        model.build_vocab(dataframe['tokens'])\n",
    "        logger.debug(f\"Количество строк в корпусе: {model.corpus_count}\")\n",
    "        logger.debug(\"Обучение...\")\n",
    "        model.train(dataframe['tokens'],  epochs=model.epochs, total_examples=model.corpus_count)\n",
    "        logger.success(\"Обучение завершено!\")\n",
    "        model_filename = f\"w2v_skipgram_{model.corpus_count}.bin\"\n",
    "        model.wv.save_word2vec_format(model_filename, binary=True)\n",
    "        logger.success(f\"Модель сохранена в {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42eafa8-f070-4d77-9501-abe1ae5136c7",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9b8988-a939-4902-a3d9-a7aa5575bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FastText_training(dataframe):\n",
    "    dataframe['tokens'] = [doc.split() for doc in tokenize(dataframe.text.tolist())]\n",
    "    tokens_col = dataframe['tokens'].tolist()\n",
    "    \n",
    "    #    Параметры модели\n",
    "    embedding_size = 100\n",
    "    window_size = 40\n",
    "    min_word = 5\n",
    "    down_sampling = 1e-2\n",
    "    \n",
    "    ft_model = FastText(tokens_col,\n",
    "                      vector_size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      epochs=20)\n",
    "    \n",
    "    semantically_similar_words = {words: [item[0] for item in ft_model.wv.most_similar([words], topn=5)]\n",
    "                  for words in ['я_NOUN', 'приехать_VERB', 'москва_PROPN', 'фотографировать_VERB', 'красиво_ADJ']}\n",
    "\n",
    "    for k,v in semantically_similar_words.items():\n",
    "        print(k+\":\"+str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee425b74-4f7b-4d44-a1a6-3390733a3814",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800db47-8a85-4016-adc5-02d977f7ca89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b8e0414-710e-4a43-9865-7845c87733bc",
   "metadata": {},
   "source": [
    "# Методы кластеризации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354c0d8-f373-4801-bc66-62d86ece708f",
   "metadata": {},
   "source": [
    "## Вычисление оптимального количества кластеров. Метод локтя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77ef7a51-6d52-4713-8383-8dc21a486aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(scaled_df):\n",
    "    kmeans_kwargs = {\n",
    "                    \"init\": \"random\",\n",
    "                    \"n_init\": 10,\n",
    "                    \"random_state\": 1,\n",
    "                    }\n",
    "    \n",
    "    #create list to hold SSE values for each k\n",
    "    sse = []\n",
    "    for k in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters = k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_df)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    \n",
    "    #visualize results\n",
    "    plt.plot(range(1, 11), sse)\n",
    "    plt.xticks(range(1, 11))\n",
    "    plt.xlabel(\"Число кластеров\")\n",
    "    plt.ylabel(\"SSE\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b162c24-5aeb-448a-8b96-8d4af9f8e3f6",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79623fbb-e911-4c3d-8bd9-867276a82b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clusterization(\n",
    "                            vectorized_text, \n",
    "                            n_clusters\n",
    "                            ):\n",
    "    #Инициализация метода главных компонент\n",
    "    kmeans = KMeans(n_clusters = n_clusters, random_state = 42)\n",
    "    clusters = kmeans.fit_predict(vectorized_text)\n",
    "    print(f\"For number of {n_clusters}:\")\n",
    "    print(f\"Silhouette Coefficient: {metrics.silhouette_score(vectorized_text, clusters):.3f}\")\n",
    "    print(f\"Inertia:{kmeans.inertia_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41224d-79d2-4d82-8d55-5e268895453f",
   "metadata": {},
   "source": [
    "## MBK-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b593a0-6da1-4812-bf8f-e4fc429744e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbkmeans_clusterization(\n",
    "        X, \n",
    "        k, \n",
    "        mb, \n",
    "        print_silhouette_values, \n",
    "    ):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters = k, batch_size = mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key =lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecda880-e74b-49d7-91e4-73e1d41c3310",
   "metadata": {},
   "source": [
    "## DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9023e5e-c97d-46a4-96f8-7ac323a0b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_clusterization(\n",
    "                            X, \n",
    "                            eps, \n",
    "                            min_samples\n",
    "                            ):\n",
    "    db = DBSCAN(eps = eps, min_samples = min_samples).fit(X)\n",
    "    labels = db.labels_\n",
    "    \n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    \n",
    "    print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "    print(f\"Silhouette Coefficient: {metrics.silhouette_score(X, labels):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca9a1c-2c70-4c5d-ac36-9414fb2c590a",
   "metadata": {},
   "source": [
    "# Чтение датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "571b6a3e-30f2-4b08-9b72-1cbc22bd6be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\u001b[32m2024-01-26 23:41:50.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[34m\u001b[1mТеги новостей: ['Первая мировая' 'Все' nan 'Прибалтика' 'Кино' 'Преступность' 'Общество'\n",
      " 'Происшествия' 'Искусство' 'Бизнес' 'Техника' 'ТВ и радио' 'Политика'\n",
      " 'Пресса' 'Музыка' 'Люди' 'Звери' 'Игры' 'Госэкономика' 'Гаджеты' 'Наука'\n",
      " 'Еда' 'Рынки' 'Деньги' 'Летние виды' 'Интернет' 'Театр' 'Конфликты'\n",
      " 'Реклама' 'Космос' 'Бокс и ММА' 'Футбол' 'Книги' 'Зимние виды'\n",
      " 'Достижения' 'Coцсети' 'Вещи' 'События' 'Средняя Азия' 'Украина'\n",
      " 'Закавказье' 'Белоруссия' 'Молдавия' 'Софт' 'Квартира' 'Город' 'Дача'\n",
      " 'Офис' 'Оружие' 'Мнения' 'Москва' 'Регионы' 'Полиция и спецслужбы'\n",
      " 'Криминал' 'Следствие и суд' 'Движение' 'Производители' 'Мировой бизнес'\n",
      " 'Финансы компаний' 'Деловой климат' 'Мир' 'Россия' 'Часы' 'Явления'\n",
      " 'Стиль' 'Инструменты' 'Вооружение' 'Вкусы' 'Страноведение'\n",
      " 'Госрегулирование' 'История' 'Внешний вид' 'Автобизнес' 'Аналитика рынка'\n",
      " 'Туризм' 'Выборы' 'Экология' 'Мемы' 'Мировой опыт' 'Инновации' 'Хоккей'\n",
      " 'Вирусные ролики' 'Фотография' 'Авто' 'Наследие' 'Преступная Россия'\n",
      " 'Жизнь' 'Киберпреступность' 'Социальная сфера' 'Казахстан'\n",
      " '69-я параллель' 'Экономика' 'Культура' 'Нацпроекты' 'Английский футбол'], Количество: 95\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 36s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news_df = pd.read_csv('lenta-ru-news.csv', delimiter =',', low_memory = True)\n",
    "logger.debug(f'Теги новостей: {news_df[\"tags\"].unique()}, Количество: {len(news_df[\"tags\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb6fd44a-541b-46db-8e40-a0ab85397b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-26 23:41:50.915\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[34m\u001b[1mТеги новостей: ['Библиотека' 'Россия' 'Мир' 'Экономика' 'Интернет и СМИ'], Количество: 5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cut_df = news_df[['text', 'topic']].head(1500)\n",
    "cut_df\n",
    "logger.debug(f'Теги новостей: {cut_df[\"topic\"].unique()}, Количество: {len(cut_df[\"topic\"].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76df81-7e45-4794-9996-2ad7edf0ad5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45e42c51-167d-4d60-81d4-ea7c87176b42",
   "metadata": {},
   "source": [
    "## TF-IDF векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e42785-9a12-4240-bb73-fee0b13521bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_vectorizer(tokenize(cut_df['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335a6ca-31eb-4554-b4b8-1512e0d23d0d",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c5ec9-062d-4e1d-aba4-16cee999e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clusterization(vectorized_text = X_tfidf,\n",
    "                      n_clusters=4\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c212d4-e714-4693-8463-536c796c3733",
   "metadata": {},
   "source": [
    "### MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af58ce-3087-4415-8dd9-ea9060dab102",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusterization(\n",
    "\tX=X_tfidf,\n",
    "    k=2,\n",
    "    mb=500,\n",
    "    print_silhouette_values=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a398dfb-bfc2-4e2c-9e9b-36c5dd269181",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c37de3-2be6-434d-bf24-592eb67ea4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_clusterization(X=X_tfidf,\n",
    "                      eps=1,\n",
    "                      min_samples= 3  \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb2ced-5145-4d8c-8945-93493ec446ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FastText_training(cut_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c4ced-bd06-451e-86ca-b8ba282e4722",
   "metadata": {},
   "source": [
    "## Word2Vec векторизацияя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104f579-f1eb-48b7-b887-1bc91c2abd26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "W2V_training(cut_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18207db1-9857-4b82-86d9-9785ec9476c9",
   "metadata": {},
   "source": [
    "### SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b029b406-3e26-4ecc-aa4c-9c094cf79c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "skipgram_vectorized_docs = W2V_vectorizer(cut_df, \"w2v_skipgram_1500.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5129a28-374d-4585-92a5-0a9ec07d3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_clusterization(X=skipgram_vectorized_docs,\n",
    "                      eps=0.3,\n",
    "                      min_samples=5\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca0ff3-24ea-4d43-a9b7-f79a3ae888bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5185001-5e04-425a-a138-eeaa977b6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusterization(\n",
    "\tX=skipgram_vectorized_docs,\n",
    "    k=4,\n",
    "    mb=400,\n",
    "    print_silhouette_values=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da1a10-4f21-4606-b847-8ef9a57e26e2",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae50270-a377-4c87-9d53-9414fe5dc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "cbow_vectorized_docs = W2V_vectorizer(cut_df, \"w2v_cbow_1500.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ff491-496f-41ab-b2d1-f1c30ead5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusterization(\n",
    "\tX=cbow_vectorized_docs,\n",
    "    k=4,\n",
    "    mb=500,\n",
    "    print_silhouette_values=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0ddde-adab-42aa-b5a1-11b5b99966b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_clusterization(X=cbow_vectorized_docs,\n",
    "                      eps=0.3,\n",
    "                      min_samples=5\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c1e8b-49e2-4e8e-b5f9-bbf65667ac40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320b4f0-d67b-4988-bb2c-7e33a6b1add6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
