{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c905266-4dc8-4b26-a627-61b089dc73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from natasha import MorphVocab\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from razdel import tokenize\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca4146-e523-418b-9d3e-ba9ae40c6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_news_df = pd.read_csv('reduced_news.csv', delimiter =',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4171c660-3163-4667-b2f1-17d4041fa5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_counts = vectorized_news_df['tags'].value_counts()\n",
    "\n",
    "min_count = 15000\n",
    "# Отфильтровываем строки, где количество категорий больше или равно min_count\n",
    "vectorized_news_df = vectorized_news_df[vectorized_news_df['tags'].isin(category_counts[category_counts > min_count].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a6d66-d42c-4a54-85aa-2e271a19e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_news_df['tags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f7616-9ca3-4fae-b48c-0fda9fe4266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "test_df = vectorized_news_df.head(150)\n",
    "test_df['target'] = labelencoder.fit_transform(test_df['tags'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4be544-6ec6-4dd0-80ac-efb294721212",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df['text'].tolist()\n",
    "Y = test_df['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6849600-e46b-4967-8393-41ce9740bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5dbfc6-8868-4337-96c0-87c10fedc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c623f-80ff-43ef-8e09-bccd142176bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fce370-406e-4791-8686-6137ca598543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "from navec import Navec\n",
    "\n",
    "# Создание объектов для сегментации и морфологического анализа\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "# Функция для лемматизации текста\n",
    "def lemmatize_text(text):\n",
    "    # Морфологический анализ сегментов\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    lemmas = []\n",
    "    vectors = []\n",
    "    for token in doc.tokens:\n",
    "        if token.pos != 'PUNCT':\n",
    "            # Получение леммы для каждого токена\n",
    "            token.lemmatize(morph_vocab)\n",
    "            lemmas.append(token.lemma)\n",
    "            if token.lemma in emb:\n",
    "                vector = emb[token.lemma]\n",
    "                vectors.append(vector)\n",
    "            else:\n",
    "                vectors.append(np.zeros(300).astype(float))\n",
    "    vectors = np.asarray(vectors)\n",
    "    vectors = vectors.mean(axis=0)\n",
    "    return vectors\n",
    "# Пример использования функции для лемматизации текста\n",
    "vectorized_news_df['vectors'] = [lemmatize_text(text) for text in tqdm(vectorized_news_df['text'])]\n",
    "vectorized_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743870e1-61e0-4a75-a501-ba0fcf26b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_news_df.to_csv('reduced_news_with_vectors.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab1b66-bbfa-429d-811f-ccb65da2ff81",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7fa0a7b-14e5-4c99-b6d6-9d1344243e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\VKR\\researches\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, Dataset\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331534ae-e295-46d2-b1ac-9fdec6c9c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_news_df = pd.read_csv('reduced_news.csv', delimiter =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc23c374-449c-4078-828a-ffd9fb996dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = vectorized_news_df['tags'].value_counts()\n",
    "\n",
    "min_count = 15000\n",
    "# Отфильтровываем строки, где количество категорий больше или равно min_count\n",
    "vectorized_news_df = vectorized_news_df[vectorized_news_df['tags'].isin(category_counts[category_counts > min_count].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cdedeb4-ebea-4c2a-8e11-bdb3441b9f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y0urc\\AppData\\Local\\Temp\\ipykernel_21784\\121829821.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['target'] = labelencoder.fit_transform(test_df['tags'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0             Общество\n",
       "1             Политика\n",
       "2             Культура\n",
       "3             Политика\n",
       "4             Общество\n",
       "            ...       \n",
       "174    Наука и техника\n",
       "175           Общество\n",
       "176           Культура\n",
       "177           Культура\n",
       "178        СВО/Украина\n",
       "Name: tags, Length: 150, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "test_df = vectorized_news_df.head(150)\n",
    "test_df['target'] = labelencoder.fit_transform(test_df['tags'])\n",
    "test_df['tags']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a7b169-20f2-4331-b2e6-1ea0ac283939",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df['text'].tolist()\n",
    "Y = test_df['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31e239c-074a-457e-8998-b17d1a658ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7777980c-1b50-43ea-8585-b83975b8ef95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased-sentence and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased-sentence', num_labels=9).to(\"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased-sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c8ab408-1281-44e7-a8cb-9cc23f091b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len_train = [len(str(i).split()) for i in X_train]\n",
    "seq_len_test = [len(str(i).split()) for i in X_test]\n",
    "max_seq_len = max(max(seq_len_test), max(seq_len_train))\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b638e975-dd46-4177-8d8a-d65e4ad6ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train,\n",
    "    max_length = max_seq_len,\n",
    "    padding = 'max_length',\n",
    "    truncation = True\n",
    ")\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    X_test,\n",
    "    max_length = max_seq_len,\n",
    "    padding = 'max_length',\n",
    "    truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f98e1833-0244-4701-8094-ae5d0264afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = Data(tokens_train, y_train)\n",
    "test_dataset = Data(tokens_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b85a1547-c700-45ac-a916-42577a62b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return {'F1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f62db658-e0cc-4ea9-b242-6561773c30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './results', #Выходной каталог\n",
    "    num_train_epochs = 3, #Кол-во эпох для обучения\n",
    "    per_device_train_batch_size = 8, #Размер пакета для каждого устройства во время обучения\n",
    "    per_device_eval_batch_size = 8, #Размер пакета для каждого устройства во время валидации\n",
    "    weight_decay =0.01, #Понижение весов\n",
    "    logging_dir = './logs', #Каталог для хранения журналов\n",
    "    load_best_model_at_end = True, #Загружать ли лучшую модель после обучения\n",
    "    learning_rate = 1e-5, #Скорость обучения\n",
    "    evaluation_strategy ='epoch', #Валидация после каждой эпохи (можно сделать после конкретного кол-ва шагов)\n",
    "    logging_strategy = 'epoch', #Логирование после каждой эпохи\n",
    "    save_strategy = 'epoch', #Сохранение после каждой эпохи\n",
    "    save_total_limit = 1,\n",
    "    seed=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b0c4fa9-b23a-4cb0-ba95-3e036202b1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\VKR\\researches\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  tokenizer = tokenizer,\n",
    "                  args = training_args,\n",
    "                  train_dataset = train_dataset,\n",
    "                  eval_dataset = train_dataset,\n",
    "                  compute_metrics = compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "591a8667-0640-4912-85a9-3518f7339920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 40:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.025300</td>\n",
       "      <td>1.926534</td>\n",
       "      <td>0.147643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.891800</td>\n",
       "      <td>1.814035</td>\n",
       "      <td>0.261807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.824400</td>\n",
       "      <td>1.772558</td>\n",
       "      <td>0.326796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=1.9138592451046674, metrics={'train_runtime': 2496.1188, 'train_samples_per_second': 0.12, 'train_steps_per_second': 0.016, 'total_flos': 58124473921800.0, 'train_loss': 1.9138592451046674, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638823ec-92e5-4840-b75b-ecc6bb5d7a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
